{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ee0b81",
      "metadata": {
        "scrolled": true,
        "id": "39ee0b81"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f4e3138",
      "metadata": {
        "id": "7f4e3138"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e993b01",
      "metadata": {
        "id": "6e993b01"
      },
      "source": [
        "# Llama 2 Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e51786c2",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "e51786c2",
        "outputId": "8f4df5a2-a1d7-45e6-d179-89c455f31e81"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not load model TheBloke/Llama-2-7B-GGML with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3453, in from_pretrained\n    raise EnvironmentError(\nOSError: TheBloke/Llama-2-7B-GGML does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-69d48ed1fc26>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TheBloke/Llama-2-7B-GGML\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TheBloke/Llama-2-7B-GGML\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not load model TheBloke/Llama-2-7B-GGML with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3453, in from_pretrained\n    raise EnvironmentError(\nOSError: TheBloke/Llama-2-7B-GGML does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n"
          ]
        }
      ],
      "source": [
        "# custom_cache_dir = \"D:/Llama2/model\"\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "# model_name = \"TheBloke/Llama-2-7B-GGML\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "#     cache_dir=custom_cache_dir,\n",
        "    token=\"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\",\n",
        "    device_map = device\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "#     cache_dir=custom_cache_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41efd1e7",
      "metadata": {
        "id": "41efd1e7"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"She is\", return_tensors=\"pt\").to(device)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e679e35e",
      "metadata": {
        "id": "e679e35e"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43be9cda",
      "metadata": {
        "id": "43be9cda"
      },
      "outputs": [],
      "source": [
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae594531",
      "metadata": {
        "id": "ae594531"
      },
      "outputs": [],
      "source": [
        "def get_llama2_reponse(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.00001)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0528ca",
      "metadata": {
        "id": "ca0528ca"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of India? A:\"\n",
        "get_llama2_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d227d52",
      "metadata": {
        "id": "1d227d52"
      },
      "outputs": [],
      "source": [
        "prompt = \"What services does SoftBuilders offer? A:\"\n",
        "get_llama2_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c211ff54",
      "metadata": {
        "id": "c211ff54"
      },
      "outputs": [],
      "source": [
        "prompt = \"How can I request a quote or discuss a project with SoftBuilders?\"\n",
        "get_llama2_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b30a0d6",
      "metadata": {
        "id": "8b30a0d6"
      },
      "outputs": [],
      "source": [
        "prompt = \"translation of sentence 'i want to eat' in hindi is\"\n",
        "get_llama2_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed12ff2",
      "metadata": {
        "id": "eed12ff2"
      },
      "outputs": [],
      "source": [
        "prompt= \"Is SoftBuilders experienced in working with businesses in various industries?\"\n",
        "print(get_llama2_reponse(prompt, max_new_tokens=50))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d27035",
      "metadata": {
        "id": "06d27035"
      },
      "source": [
        "# Llama 2 Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5da454",
      "metadata": {
        "id": "3e5da454"
      },
      "outputs": [],
      "source": [
        "chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f5b55c",
      "metadata": {
        "id": "a5f5b55c"
      },
      "outputs": [],
      "source": [
        "def get_llama2_chat_reponse(prompt, max_new_tokens=50):\n",
        "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = chat_model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.00001)\n",
        "    response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856bc627",
      "metadata": {
        "id": "856bc627"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q:what is the capital of India? A:\"\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bedc6b7",
      "metadata": {
        "id": "2bedc6b7"
      },
      "outputs": [],
      "source": [
        "prompt = \"What services does SoftBuilders offer? A:\"\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cc0d49",
      "metadata": {
        "id": "a9cc0d49"
      },
      "outputs": [],
      "source": [
        "prompt='''python code to loop from 1 to 10 and print the numbers is:'''\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8158d2",
      "metadata": {
        "id": "8c8158d2"
      },
      "outputs": [],
      "source": [
        "prompt = \"Is SoftBuilders experienced in working with businesses in various industries?\"\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ea2313",
      "metadata": {
        "id": "e4ea2313"
      },
      "source": [
        "## After Some Instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cb2aea",
      "metadata": {
        "id": "67cb2aea"
      },
      "outputs": [],
      "source": [
        "prompt = \"Keep answer short. Q: what is the capital of India?\"\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079f49a4",
      "metadata": {
        "id": "079f49a4"
      },
      "outputs": [],
      "source": [
        "prompt = \"[INST] Is SoftBuilders experienced in working with businesses in various industries? [/INST]\"\n",
        "get_llama2_chat_reponse(prompt, max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a582309",
      "metadata": {
        "id": "5a582309"
      },
      "outputs": [],
      "source": [
        "prompt = \"[INST] What services does SoftBuilders offer? [/INST]\"\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7069c5",
      "metadata": {
        "id": "9a7069c5"
      },
      "outputs": [],
      "source": [
        "prompt = \"[INST] How can I request a quote or discuss a project with SoftBuilders? [/INST]\"\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028cd8aa",
      "metadata": {
        "id": "028cd8aa"
      },
      "source": [
        "## Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850a5bbc",
      "metadata": {
        "id": "850a5bbc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ecf322",
      "metadata": {
        "id": "46ecf322"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff52a71",
      "metadata": {
        "id": "6ff52a71"
      },
      "outputs": [],
      "source": [
        "def get_llama2_chat_reponse(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature= 0.001)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ddeafc",
      "metadata": {
        "id": "90ddeafc"
      },
      "source": [
        "## Zero Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aba5a4",
      "metadata": {
        "id": "01aba5a4"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST]\n",
        "Classify the text into neutral, negative or positive.\n",
        "Text: I just love it\n",
        "[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921d30aa",
      "metadata": {
        "id": "921d30aa"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST]\n",
        "what is the hindi translation of the text\n",
        "Text: I like eating\n",
        "[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd09b9b",
      "metadata": {
        "id": "3dd09b9b"
      },
      "source": [
        "## Few Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be08ced1",
      "metadata": {
        "id": "be08ced1"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
        "the word farduddle is:  [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa643b56",
      "metadata": {
        "id": "fa643b56"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
        "the word whatpu is: [/INST]\n",
        "We were traveling in Africa and we saw these very cute whatpus.\n",
        "[INST] To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
        "the word farduddle is: [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792850c6",
      "metadata": {
        "id": "792850c6"
      },
      "source": [
        "## Few Shot Prompting for Strcuture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0dad2e",
      "metadata": {
        "id": "3d0dad2e"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] <<SYS>>\n",
        "Classify the text into neutral, negative or positive.\n",
        "<</SYS>>\n",
        "\n",
        "I like this.[/INST]\n",
        "positive\n",
        "[INST] I hate this [/INST]\n",
        "negative\n",
        "[INST] this is ok [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc344fea",
      "metadata": {
        "id": "cc344fea"
      },
      "source": [
        "## Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9987e142",
      "metadata": {
        "id": "9987e142"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] True or False. The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82. [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3260a3ad",
      "metadata": {
        "id": "3260a3ad"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. [/INST]\n",
        "A: Odd numbers in the group are: 9, 15, 1. Sum of them is 25. 25 is odd. The answer is False.\n",
        "[INST] The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82. [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afeb54ef",
      "metadata": {
        "id": "afeb54ef"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] True or False?\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82.\n",
        "Let's think step by step. [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b16c3b",
      "metadata": {
        "id": "a0b16c3b"
      },
      "source": [
        "## Prompting for Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e506a597",
      "metadata": {
        "id": "e506a597"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] <<SYS>>\n",
        "Respond only by quoting from the TV series Friends\n",
        "<</SYS>>\n",
        "\n",
        "Hi[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267b3625",
      "metadata": {
        "id": "267b3625"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] <<SYS>>\n",
        "Respond only by quoting from the TV series Friends\n",
        "<</SYS>>\n",
        "\n",
        "Hi[/INST]\n",
        "\"How you doin'?\"\n",
        "[INST] whats the capital of India?[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0160f81",
      "metadata": {
        "id": "b0160f81"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8f22c0",
      "metadata": {
        "id": "bb8f22c0"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] <<SYS>>\n",
        "Respond only by using emojis\n",
        "<</SYS>>\n",
        "\n",
        "Hi[/INST]\n",
        "ðŸ‘‹\n",
        "[INST] whats the capital of India?[/INST]\n",
        "ðŸ‡®ðŸ‡³\n",
        "[INST] show me some dance move [/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc4452a",
      "metadata": {
        "id": "1fc4452a"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST]\n",
        "Hi, my name is Ruhullah and I am 23[/INST]\n",
        "Hello Ruhullah\n",
        "[INST] whats the capital of India?[/INST]\n",
        "Delhi\n",
        "[INST] whats my name? [/INST]\n",
        "Your name is Ruhullah.\n",
        "[INST] what is my age?[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f0efb2",
      "metadata": {
        "id": "a4f0efb2"
      },
      "source": [
        "## Example on softbuilders FAQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188d627b",
      "metadata": {
        "id": "188d627b"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "[INST] What is SoftBuilders?[/INST]\n",
        "SoftBuilders is a software Company. It provides various services including web development, app development, UI/UX design, machine learning and Block chain.\n",
        "[INST] What is Services SoftBuilders?[/INST]\n",
        "It provides various services including web development, app development, UI/UX design, machine learning and Block chain.\n",
        "[INST] where it is located? [/INST]\n",
        "SoftBuilders is a UAE based company, located at Floor 20th, The Exchange Tower, Business Bay, Business Bay Dubai (UAE).\n",
        "[INST] Who is the CEO of SoftBuilders?[/INST]\n",
        "Alex Vicini\n",
        "[INST] Who is the AI member of SoftBuilders?[/INST]\n",
        "Nabeel, Ali and Ruhullah.\n",
        "[INST] What services does SoftBuilders offer?[/INST]\n",
        "SoftBuilders specializes in providing custom software development solutions tailored to meet your business needs.\n",
        "We offer a wide range of services, including web application development, mobile app development, software consulting, and IT support.\n",
        "[INST] How can I request a quote or discuss a project with SoftBuilders?[/INST]\n",
        "To request a quote or discuss a project, you can reach out to us through our contact form on the website, send us an email at admin@softbuilders.com, or give us a call at +971502891468.\n",
        "Our team will promptly respond to your inquiry and guide you through the process.\n",
        "[INST] Is SoftBuilders experienced in working with businesses in various industries?[/INST]\n",
        "Yes, SoftBuilders has extensive experience working with businesses across diverse industries, including finance, healthcare, e-commerce, logistics, mlm , dropshhiping and more.\n",
        "Our team of skilled professionals understands the unique challenges and requirements of different sectors, enabling us to deliver tailored solutions to meet your industry-specific needs..\n",
        "[INST] What is the Missions of the company?[/INST]\n",
        "At Softbuilders Software Design LLC, our mission is to provide our clients with exceptional software design and development services that exceed their expectations. We aim to build long-lasting partnerships by delivering high-quality, reliable, and scalable solutions that help businesses thrive in the digital landscape\n",
        "[INST] What is the Vision of the company?[/INST]\n",
        "Our vision is to be a leading software design and development company, recognized for our technical expertise, innovative solutions, and unwavering commitment to customer satisfaction. We aim to empower businesses with state-of-the-art software solutions that drive growth, efficiency, and success.\n",
        "[INST] What is the address of the company?[/INST]\n",
        "'''\n",
        "print(get_llama2_chat_reponse(prompt, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c7d796",
      "metadata": {
        "id": "c1c7d796"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdb4fa4",
      "metadata": {
        "id": "cfdb4fa4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfbd3a8b",
      "metadata": {
        "id": "bfbd3a8b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78deb51",
      "metadata": {
        "id": "b78deb51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08eaad1a",
      "metadata": {
        "id": "08eaad1a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b7e8e8",
      "metadata": {
        "id": "08b7e8e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c175bdc",
      "metadata": {
        "id": "0c175bdc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d07e03",
      "metadata": {
        "id": "b5d07e03"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9845b2",
      "metadata": {
        "id": "4d9845b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55ee87d",
      "metadata": {
        "id": "c55ee87d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e57ca2d",
      "metadata": {
        "id": "0e57ca2d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}